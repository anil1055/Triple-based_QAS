{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Download NLTK libraries"
      ],
      "metadata": {
        "id": "TqUw3k9oD-jU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "HSn0Euzs_sAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Punction, sentence tokenizer, pos tagging, lemmatization process"
      ],
      "metadata": {
        "id": "GLkOy_yMEEAj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "sgjDG3eUyQpF"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def punctionProcess(text):\n",
        "    out = str(text).maketrans('','', string.punctuation)\n",
        "    result = str(text).translate(out)\n",
        "    \n",
        "    return result\n",
        "\n",
        "#punktTokenizer for sentence detection\n",
        "def punktSentenceTokenizer(paragraph):\n",
        "    sent_tokenizer = nltk.tokenize.PunktSentenceTokenizer()\n",
        "    sentences = sent_tokenizer.tokenize(paragraph)\n",
        "    updateSentence = solveSentenceTokenizer(sentences)\n",
        "    if len(updateSentence) == 0:\n",
        "        updateSentence = sentences\n",
        "\n",
        "    return updateSentence\n",
        "\n",
        "\n",
        "#POS Tagging\n",
        "def posTag(text):\n",
        "    txt = word_tokenize(text)\n",
        "    pos = nltk.pos_tag(txt)\n",
        "\n",
        "    return pos\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Lemmatize with POS Tag\n",
        "from nltk.corpus import wordnet\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "\n",
        "#Lemmatization process\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "def lemmaProcess(sentence):       \n",
        "    input_str=word_tokenize(sentence)\n",
        "    txt = ''\n",
        "    for word in input_str:\n",
        "        txt += lemmatizer.lemmatize(word, get_wordnet_pos(word)) + ' ' #, get_wordnet_pos(word)\n",
        "    sentence = str(txt).strip()\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "#developed solve for sentence tokenizer error\n",
        "def solveSentenceTokenizer(sentences):\n",
        "    firstStage = []\n",
        "    ind = 0\n",
        "    indList = []\n",
        "    last = []\n",
        "    #If next sentence first word's first letter is lower or numeric char, combine with sentences\n",
        "    s = ''\n",
        "    for sentence in sentences:\n",
        "        if ind+1 < len(sentences):\n",
        "            words = word_tokenize(sentences[ind+1])\n",
        "            word = words[0]\n",
        "            if str(word).islower() == True or str(word).isnumeric() == True:\n",
        "                firstStage.append(sentence+ ' ' + sentences[ind+1])\n",
        "                s += sentence+ ' ' + sentences[ind+1]\n",
        "                last.append(ind+1)\n",
        "                if ind+1 - last[len(last)-2] == 1:\n",
        "                    sent = firstStage[len(firstStage)-2]\n",
        "                    firstStage.remove(firstStage[len(firstStage)-2])\n",
        "                    firstStage.remove(firstStage[len(firstStage)-1])\n",
        "                    sent += ' ' + sentences[ind+1]\n",
        "                    firstStage.append(sent)\n",
        "                    s += sent\n",
        "            else:\n",
        "                firstStage.append(sentence)  \n",
        "        else:\n",
        "                firstStage.append(sentence) \n",
        "        ind += 1\n",
        "    for ind in range(0, len(firstStage)-1):\n",
        "        if str(firstStage[ind]).find(firstStage[ind+1]) != -1:\n",
        "            indList.append(ind+1)\n",
        "    indList.sort(reverse=True)\n",
        "    for ind in indList:\n",
        "        firstStage.remove(firstStage[ind])\n",
        "    #If sentence containing max 2 word, combines with next sentences\n",
        "    secondStage = []\n",
        "    ind = 0\n",
        "    for sentence in firstStage:\n",
        "        words = word_tokenize(sentence) \n",
        "        if len(words) < 3:\n",
        "            if  ind+1 < len(firstStage):\n",
        "                secondStage.append(sentence + ' ' + firstStage[ind+1])\n",
        "        else:\n",
        "            if len(secondStage) > 0:\n",
        "                if str(secondStage[len(secondStage)-1]).find(sentence) == -1:\n",
        "                    secondStage.append(sentence)\n",
        "            else:\n",
        "                secondStage.append(sentence)\n",
        "        ind += 1\n",
        "    #Previous word is uppercase, but last word's first character is uppercase with max 3 length => combine with next sentences \n",
        "    lastSentences = []\n",
        "    ind = 0\n",
        "    for update in secondStage:\n",
        "        words = word_tokenize(update) \n",
        "        if len(words) >= 2:\n",
        "            word = words[len(words)-2]\n",
        "            if len(lastSentences) != 0:\n",
        "                if str(lastSentences[len(lastSentences)-1]).find(update) != -1:\n",
        "                    ind += 1\n",
        "                    continue\n",
        "            if str(word[0]).isupper() == True and len(word) < 4 and len(words) >= 3:\n",
        "                word = words[len(words)-3]\n",
        "                if str(word[0]).islower() == True and len(secondStage) > ind+1:\n",
        "                    lastSentences.append(update + ' ' + secondStage[ind+1])\n",
        "                else:\n",
        "                    lastSentences.append(update)\n",
        "            else:\n",
        "                lastSentences.append(update)\n",
        "        ind += 1\n",
        "    return lastSentences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adding StanfordOpenIE library"
      ],
      "metadata": {
        "id": "6-sKS-PEFyub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget\n",
        "!pip install stanfordnlp\n",
        "#!pip install neuralcoref==4.0"
      ],
      "metadata": {
        "id": "URqjFFykLts9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "from subprocess import Popen\n",
        "from sys import stderr\n",
        "from zipfile import ZipFile\n",
        "import wget\n",
        "from spacy.lang.en import English "
      ],
      "metadata": {
        "id": "rjrJdpUgF7RS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StanfordOpenIE:\n",
        "\n",
        "    def __init__(self, core_nlp_version: str = '4.1.0'):\n",
        "        self.install_dir = Path('~/.stanfordnlp_resources/').expanduser()\n",
        "        self.install_dir.mkdir(exist_ok=True)\n",
        "        if len([d for d in self.install_dir.glob('*') if d.is_dir()]) == 0:\n",
        "            # No coreNLP directories. Let's check for ZIP archives as well.\n",
        "            zip_files = [d for d in self.install_dir.glob('*') if d.suffix == '.zip']\n",
        "            if len(zip_files) == 0:\n",
        "                # No dir and no ZIP. Let's download it with the desired core_nlp_version.\n",
        "                remote_url = 'https://nlp.stanford.edu/software/stanford-corenlp-{}.zip'.format(core_nlp_version)\n",
        "                print('Downloading from %s.' % remote_url)\n",
        "                output_filename = wget.download(remote_url, out=str(self.install_dir))\n",
        "                print('\\nExtracting to %s.' % self.install_dir)\n",
        "            else:\n",
        "                output_filename = zip_files[0]\n",
        "            print('Unzip %s.' % output_filename)\n",
        "            zf = ZipFile(output_filename)\n",
        "            zf.extractall(path=self.install_dir)\n",
        "            zf.close()\n",
        "        target_dir = [d for d in self.install_dir.glob('*') if d.is_dir()][0]\n",
        "\n",
        "        os.environ['CORENLP_HOME'] = str(self.install_dir / target_dir)\n",
        "        from stanfordnlp.server import CoreNLPClient\n",
        "        self.client = CoreNLPClient(annotators=['openie'], memory='8G')\n",
        "\n",
        "    def annotate(self, text: str, properties_key: str = None, properties: dict = None, simple_format: bool = True):\n",
        "        \"\"\"\n",
        "        :param (str | unicode) text: raw text for the CoreNLPServer to parse\n",
        "        :param (str) properties_key: key into properties cache for the client\n",
        "        :param (dict) properties: additional request properties (written on top of defaults)\n",
        "        :param (bool) simple_format: whether to return the full format of CoreNLP or a simple dict.\n",
        "        :return: Depending on simple_format: full or simpler format of triples <subject, relation, object>.\n",
        "        \"\"\"\n",
        "        # https://stanfordnlp.github.io/CoreNLP/openie.html\n",
        "        core_nlp_output = self.client.annotate(text=text, annotators=['openie'], output_format='json',\n",
        "                                               properties_key=properties_key, properties=properties)\n",
        "        if simple_format:\n",
        "            triples = []\n",
        "            for sentence in core_nlp_output['sentences']:\n",
        "                for triple in sentence['openie']:\n",
        "                    triples.append({\n",
        "                        'subject': triple['subject'],\n",
        "                        'relation': triple['relation'],\n",
        "                        'object': triple['object']\n",
        "                    })\n",
        "            return triples\n",
        "        else:\n",
        "            return core_nlp_output\n",
        "\n",
        "    def generate_graphviz_graph(self, text: str, png_filename: str = './out/graph.png'):\n",
        "        \"\"\"\n",
        "       :param (str | unicode) text: raw text for the CoreNLPServer to parse\n",
        "       :param (list | string) png_filename: list of annotators to use\n",
        "       \"\"\"\n",
        "        entity_relations = self.annotate(text, simple_format=True)\n",
        "        \"\"\"digraph G {\n",
        "        # a -> b [ label=\"a to b\" ];\n",
        "        # b -> c [ label=\"another label\"];\n",
        "        }\"\"\"\n",
        "        graph = list()\n",
        "        graph.append('digraph {')\n",
        "        for er in entity_relations:\n",
        "            graph.append('\"{}\" -> \"{}\" [ label=\"{}\" ];'.format(er['subject'], er['object'], er['relation']))\n",
        "        graph.append('}')\n",
        "\n",
        "        output_dir = os.path.join('.', os.path.dirname(png_filename))\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        out_dot = os.path.join(tempfile.gettempdir(), 'graph.dot')\n",
        "        with open(out_dot, 'w') as output_file:\n",
        "            output_file.writelines(graph)\n",
        "\n",
        "        command = 'dot -Tpng {} -o {}'.format(out_dot, png_filename)\n",
        "        dot_process = Popen(command, stdout=stderr, shell=True)\n",
        "        dot_process.wait()\n",
        "        assert not dot_process.returncode, 'ERROR: Call to dot exited with a non-zero code status.'\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        pass\n",
        "\n",
        "    def __del__(self):\n",
        "        self.client.stop()\n",
        "        del os.environ['CORENLP_HOME']"
      ],
      "metadata": {
        "id": "dbTC9KMAF3JS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo Process"
      ],
      "metadata": {
        "id": "9tZFfI2SHGVq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Triple extraction with OpenIE"
      ],
      "metadata": {
        "id": "3XQqKrCdHRXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coreference Resolution"
      ],
      "metadata": {
        "id": "MNvzs3HeHWwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy==2.1.0\n",
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz"
      ],
      "metadata": {
        "id": "4TrnYQlYPBAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huggingface/neuralcoref.git"
      ],
      "metadata": {
        "id": "vqtW_NB9NX4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neuralcoref==4.0"
      ],
      "metadata": {
        "id": "h2gykLbGbNgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd neuralcoref/\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "l0-2-R_hIwEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy validate"
      ],
      "metadata": {
        "id": "99-68_A0R-kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import neuralcoref\n",
        "\n",
        "coreference_model = spacy.load('en_core_web_sm')\n",
        "neuralcoref.add_to_pipe(coreference_model)"
      ],
      "metadata": {
        "id": "kcoj8VibIUyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = coreference_model('My sister has a dog. She loves him.')\n",
        "print(doc1._.coref_clusters)"
      ],
      "metadata": {
        "id": "qN2UUSeIZZ5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def coreferenceResolution(paragraph, method):\n",
        "    try:\n",
        "        try:\n",
        "            while paragraph.index('(') != -1:\n",
        "                lindx = paragraph.index('(')\n",
        "                rindx = paragraph.index(')', lindx)\n",
        "                paragraph = paragraph[:lindx].strip() + ' ' + paragraph[rindx+1:].lstrip()\n",
        "        except:\n",
        "            paragraph = paragraph       \n",
        "        sentences = punktSentenceTokenizer(paragraph)\n",
        "        newParagraph = ''\n",
        "        for sent in sentences:\n",
        "            newParagraph += str(sent[:-1]) + '.. '\n",
        "        print(newParagraph)\n",
        "        if method == 1: #HuggingFace                       \n",
        "            doc = coreference_model(newParagraph)  # get the spaCy Doc (composed of Tokens)\n",
        "            paragraph = doc._.coref_resolved\n",
        "        newSentence = paragraph.split('.. ')\n",
        "        print(paragraph)\n",
        "        sentences = []\n",
        "        paragraph = ''\n",
        "        for sentence in newSentence:    \n",
        "            paragraph += sentence[:1].capitalize() + sentence[1:] + '. '\n",
        "            sentences.append(sentence[:1].capitalize() + sentence[1:] + '. ')\n",
        "        return sentences, paragraph[:-3]\n",
        "    except Exception as e:\n",
        "        print('ErrorCorefRes:' + str(e))\n",
        "        return paragraph, \"\""
      ],
      "metadata": {
        "id": "OycAajgUHM_6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wordProcess(question):\n",
        "    from nltk.corpus import stopwords \n",
        "    from nltk.tokenize import word_tokenize\n",
        "    stop_words = stopwords.words('english') \n",
        "    word_tokens = word_tokenize(question)\n",
        "    filtered_ques = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "    ansType = NERstatement(question)\n",
        "    question = ''\n",
        "    for q in filtered_ques:\n",
        "        question += q + ' '\n",
        "    question = question[:-1].strip()\n",
        "\n",
        "    return question, ansType"
      ],
      "metadata": {
        "id": "TTVKRFjFHbxJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NERstatement(ques):\n",
        "    ansType = ''\n",
        "    if ques.find('who') != -1:\n",
        "        ansType = 'PERSON'\n",
        "    if ques.find('when') != -1:\n",
        "        ansType = 'DATE'\n",
        "    if ques.find('how many') != -1:\n",
        "        ansType = 'QUANTITY'\n",
        "    if ques.find('how much') != -1:\n",
        "        ansType = 'QUANTITY, MONEY'\n",
        "    if ques.find('where') != -1:\n",
        "        ansType = 'GPE'\n",
        "    pos = posTag(ques)\n",
        "    if len(pos)> 2:\n",
        "        for i in range(len(pos)-1):\n",
        "            if pos[i][0] == 'what' or pos[i][0] == 'which' or pos[i][0] == 'where' or pos[i][0] == 'whose':\n",
        "                if pos[i+1][0] == 'people' or pos[i+1][0] == 'person':\n",
        "                    ansType = 'PERSON'\n",
        "                if pos[i+1][0] == 'nationality' or pos[i+1][0] == 'religion' or pos[i+1][0] == 'politic':\n",
        "                    ansType = 'NORP'\n",
        "                if pos[i+1][0] == 'country' or pos[i+1][0] == 'city' or pos[i+1][0] == 'state':\n",
        "                    ansType = 'GPE'\n",
        "                if pos[i+1][0] == 'company' or pos[i+1][0] == 'agency' or pos[i+1][0] == 'institution':\n",
        "                    ansType = 'ORG'\n",
        "                if pos[i+1][0] == 'building' or pos[i+1][0] == 'airport' or pos[i+1][0] == 'highway' or pos[i+1][0] == ' bridge':\n",
        "                    ansType = 'FAC'\n",
        "                if pos[i+1][0] == 'location' or pos[i+1][0] == 'range':\n",
        "                    ansType = 'LOC'\n",
        "                if pos[i+1][0] == 'hurricane' or pos[i+1][0] == 'battle' or pos[i+1][0] == 'war':\n",
        "                    ansType = 'EVENT'\n",
        "                if pos[i+1][0] == 'song' or pos[i+1][0] == 'title':\n",
        "                    ansType = 'WORK_OF_ART'\n",
        "                if pos[i+1][0] == 'language':\n",
        "                    ansType = 'LANGUAGE'\n",
        "                if pos[i+1][0] == 'time' or pos[i+1][0] == 'date' or pos[i+1][0] == 'period' or pos[i+1][0] == 'duration' or pos[i+1][0] == 'year' or pos[i+1][0] == 'month' or pos[i+1][0] == 'day':\n",
        "                    ansType = 'DATE'\n",
        "                if pos[i+1][0] == 'percentage':\n",
        "                    ansType = 'PERCENT'\n",
        "                if pos[i+1][0] == 'measurement' or pos[i+1][0] == 'weight' or pos[i+1][0] == 'distance':\n",
        "                    ansType = 'QUANTITY'\n",
        "    return ansType\n",
        "\n",
        "nlp_model = spacy.load('en_core_web_sm')\n",
        "def NERdetection(sentence, ansType):\n",
        "    sentence = nlp_model(sentence)\n",
        "    labels = [x.label_ for x in sentence.ents]\n",
        "    answer_cont = False\n",
        "    if len(labels) != 0:\n",
        "        for i in range(0, len(labels)):\n",
        "            if ansType.find(labels[i]) != -1:\n",
        "                answer_cont = True\n",
        "                break\n",
        "    return answer_cont, labels"
      ],
      "metadata": {
        "id": "ZaZwZNd-H0Bp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def OpenIEforSentence(sentence, client):\n",
        "    svoText = []\n",
        "    print('OpenIE entered')\n",
        "    for triple in client.annotate(sentence, properties={\n",
        "    'timeout': '500000'}):\n",
        "        svoText.append(str(triple))\n",
        "    return svoText"
      ],
      "metadata": {
        "id": "HYTZ1sQKHQlK"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def demo(ques, parag_examp, actual_ans, sel_sentence, no_answer = False, preprocess = False):     \n",
        "    \n",
        "    with StanfordOpenIE() as StanfordClient:\n",
        "        svoList = []\n",
        "        corefSent = ''\n",
        "        corefParag = ''\n",
        "        sub_rel_obj = ''\n",
        "        processes = ''\n",
        "        if no_answer == False:\n",
        "            paragraph = parag_examp\n",
        "            actual_answer = actual_ans\n",
        "            sentence = sel_sentence\n",
        "            question = ques.lower()\n",
        "            paragraph = str(paragraph).replace('\\n', '')\n",
        "            totalFindAnswer = 0\n",
        "            findAns = 0\n",
        "            sentences = punktSentenceTokenizer(paragraph)\n",
        "            sind = -1\n",
        "            sPosition = 0\n",
        "            try:\n",
        "                for sent in sentences:\n",
        "                    if str(sent).find(str(sentence).strip()) != -1:\n",
        "                        sPosition = sind + 1\n",
        "                        sind += 1\n",
        "                        break\n",
        "                    sind += 1    \n",
        "                if sind == -1:\n",
        "                    print('Finding sentence index error')\n",
        "                else:\n",
        "                    corefSent, corefParag = coreferenceResolution(parag_examp, 1)\n",
        "                    print(corefSent, corefParag)\n",
        "                    svoList = OpenIEforSentence(corefSent[sPosition], StanfordClient)\n",
        "                    corefSent = corefSent[sPosition]\n",
        "            except Exception as e:\n",
        "                print(\"Error LocationCoref:\" + str(e))\n",
        "\n",
        "            answerList = []            \n",
        "            try:\n",
        "                for triple in svoList:                 \n",
        "                    triple = str(triple).split(',')\n",
        "                    subject = str(str(triple[0]).split(':')[1]).strip()[1:-1].lower()            \n",
        "                    relation = str(' ' + str(str(triple[1]).split(':')[1]).strip()[1:-1] + ' ').lower()\n",
        "                    object = str(str(triple[2]).split(':')[1]).strip()[1:-2].lower()\n",
        "                    if preprocess:\n",
        "                        subject, ansType = wordProcess(subject)\n",
        "                        relation, ansType = wordProcess(relation)\n",
        "                        object, ansType = wordProcess(object)\n",
        "                    answer = ''\n",
        "                    if str(question).find(subject) != -1 and str(question).find(relation) != -1:\n",
        "                        if str(question).find(object) == -1:\n",
        "                            answer = object\n",
        "                            sub_rel_obj = triple\n",
        "                    if str(question).find(object) != -1 and str(question).find(relation) != -1:\n",
        "                        if str(question).find(subject) == -1:    \n",
        "                            answer = subject\n",
        "                            sub_rel_obj = triple\n",
        "                    if str(question).find(object) != -1 and str(question).find(subject) != -1:\n",
        "                        if str(question).find(relation) == -1:    \n",
        "                            answer = relation\n",
        "                            sub_rel_obj = triple\n",
        "                    if answer == actual_answer:\n",
        "                        totalFindAnswer += 1\n",
        "                    if answer != '':\n",
        "                        answerList.append(answer)       \n",
        "            except Exception as e:\n",
        "                print(\"Error searchSvo:\" + str(e))\n",
        "\n",
        "            if len(answerList) == 1 and findAns != totalFindAnswer:\n",
        "                print(\"Triple found successfully! Answer: \" + str(answerList[0]))  \n",
        "\n",
        "        else: \n",
        "            paragraph = parag_examp\n",
        "            question = ques.lower()              \n",
        "            paragraph = str(paragraph).replace('\\n', '')\n",
        "            triples = []\n",
        "            corefSent, corefParag = coreferenceResolution(parag_examp, 1)\n",
        "            for sent in corefSent:\n",
        "                svoList = OpenIEforSentence(sent, StanfordClient)\n",
        "                for svo in svoList:\n",
        "                    triples.append(svo)\n",
        "            corefSent = ''\n",
        "            svoList = triples\n",
        "            question, answerType = wordProcess(question)\n",
        "            processes += 'NER answer label: ' + answerType + '\\n'\n",
        "            answer_cont = False\n",
        "            labels = []\n",
        "            try:\n",
        "                for triple in triples: \n",
        "                    triple = str(triple).split(',')\n",
        "                    subject = str(str(triple[0]).split(':')[1]).strip()[1:-1]\n",
        "                    relation = str(' ' + str(str(triple[1]).split(':')[1]).strip()[1:-1] + ' ')\n",
        "                    object = str(str(triple[2]).split(':')[1]).strip()[1:-2]\n",
        "                    if preprocess:\n",
        "                        subject, ansType = wordProcess(subject)\n",
        "                        relation, ansType = wordProcess(relation)\n",
        "                        object, ansType = wordProcess(object)\n",
        "                    if str(question).find(subject.lower()) != -1 and str(question).find(relation.lower()) != -1:\n",
        "                        if answerType != '':\n",
        "                            answer_cont, labels = NERdetection(object.strip(), answerType)\n",
        "                            processes += str(labels) + '\\n'\n",
        "                        else: answer_cont = True\n",
        "                    if str(question).find(object.lower()) != -1 and str(question).find(relation.lower()) != -1:   \n",
        "                        if answerType != '':\n",
        "                            answer_cont, labels = NERdetection(subject.strip(), answerType)\n",
        "                            processes += str(labels) + '\\n'\n",
        "                        else: answer_cont = True\n",
        "                    if str(question).find(object.lower()) != -1 and str(question).find(subject.lower()) != -1: \n",
        "                        if answerType != '':\n",
        "                            answer_cont, labels = NERdetection(relation.strip(), answerType)\n",
        "                            processes += str(labels) + '\\n'\n",
        "                        else: answer_cont = True\n",
        "                    if answer_cont == True:\n",
        "                        break\n",
        "                if answer_cont == False:                    \n",
        "                    print(\"Triple not found successfully!\")  \n",
        "      \n",
        "            except Exception as e:\n",
        "                print(\"Error searchSvo:\" + str(e))\n",
        "\n",
        "\n",
        "        StanfordClient.client.stop()\n",
        "        return svoList, corefParag, corefSent, sub_rel_obj, processes"
      ],
      "metadata": {
        "id": "Rp4qgLlsHxOK"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def defineQuestionTerm(preProcess):\n",
        "    questionSearch = ''\n",
        "    for terms in preProcess:\n",
        "        for term in terms:\n",
        "            if str(term).find('_') != -1:\n",
        "                term = str(term).replace('_',' ')\n",
        "            questionSearch += term + ','\n",
        "    questionSearch = questionSearch[:-1]\n",
        "    return questionSearch"
      ],
      "metadata": {
        "id": "Z6vGPKbP5qlI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sorted all sentence according to QTP value"
      ],
      "metadata": {
        "id": "TUUXhrECEh3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def findPath(rateList,textList):\n",
        "    sortList = []\n",
        "    sortTxt = ''\n",
        "    for rate in rateList:\n",
        "        value = max(rateList)\n",
        "        ind = rateList.index(value)\n",
        "        sortList.append(ind)\n",
        "        rateList[ind] = 0\n",
        "    for sort in sortList:\n",
        "        sortTxt += textList[sort] + '\\n'\n",
        "    return sortTxt"
      ],
      "metadata": {
        "id": "sSgzmDvx-sO1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Search question words in sentences with question term percentage(QTP)"
      ],
      "metadata": {
        "id": "BT9L2ApmEv3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def searchWord(text,paragraph,lemma):\n",
        "    texts = []\n",
        "    textSearch = ''\n",
        "    path = ''\n",
        "    try:\n",
        "        if str(text).find(',') != -1:\n",
        "            texts = str(text).split(',')\n",
        "        else:\n",
        "            texts.append(text)  \n",
        "        par = paragraph          \n",
        "        sentences = punktSentenceTokenizer(par)\n",
        "        s_ind = 0\n",
        "        rateList = []\n",
        "        textList = []\n",
        "        for sentence in sentences:         \n",
        "            sent = punctionProcess(str(sentence))\n",
        "            if lemma == True:\n",
        "                sent = lemmaProcess(sent)\n",
        "                sentence = sent\n",
        "            c = 0\n",
        "            qaText = [] \n",
        "            qTerm = []\n",
        "            for i in range(0,len(texts)):\n",
        "                texts[i] = str(texts[i]).strip()\n",
        "                if str(sent.lower()).find(texts[i].lower()) != -1:\n",
        "                    c += 1\n",
        "                    qTerm.append(str(texts[i]))\n",
        "\n",
        "            if c != 0:\n",
        "                textSearch += 'Find question:' + str(qTerm) + ', QCount:' + str(c) + '\\nQuestion rate:%' + str(c/len(texts)*100) + '\\n' + str(s_ind+1) + '. sentence - ' + sentence + '\\n\\n'\n",
        "                rateList.append(c/len(texts)*100)\n",
        "                textList.append('Find question:' + str(qTerm) + ', QCount:' + str(c) + '\\nQuestion rate:%' + str(c/len(texts)*100) + '\\n' +  str(s_ind+1) + '. sentence - ' + sentence + '\\n\\n')\n",
        "            s_ind += 1\n",
        "        if len(rateList) != 0:\n",
        "            path += findPath(rateList,textList)\n",
        "    except Exception as e:\n",
        "        print('SearchWord:' + str(e) + ', text:' + str(texts))\n",
        "    return textSearch, path"
      ],
      "metadata": {
        "id": "uB56oRZN6ClP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph_examples = 'Ege University is a public research university in İzmir, Turkey. It was founded in 1955. It is the first university to start courses in İzmir and the fourth oldest university in Turkey. Ege University in Bornova, a district of Izmir, the third largest city in Turkey.'\n",
        "answer_examples = ['1955', 'No Answer']\n",
        "question_examples = ['When was Ege University founded?','What is the second oldest university in Turkey?']\n",
        "\n",
        "paragraph = paragraph_examples"
      ],
      "metadata": {
        "id": "9ohL5hF2IXn5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question that has an answer"
      ],
      "metadata": {
        "id": "ye20FRx1hkZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "StanfordOpenIE has a timeout error in Google Colab, please you'll run your local repository."
      ],
      "metadata": {
        "id": "JPhBNnlAvR-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = question_examples[0]\n",
        "answer = answer_examples[0]\n",
        "\n",
        "question = punctionProcess(question)\n",
        "X_list = word_tokenize(question) \n",
        "preProcess = [X_list]       \n",
        "questionSearch = defineQuestionTerm(preProcess)  \n",
        "textSearch, path = searchWord(questionSearch, paragraph, False)\n",
        "texts = path.split('\\n')\n",
        "\n",
        "sel_sentence = ''\n",
        "for txt in texts:\n",
        "    if (str(txt).find(answer.lower()) != -1 or str(txt).find(answer) != -1):\n",
        "        sentence = str(txt).split('sentence - ')\n",
        "        sentence = str(sentence[1]).strip()\n",
        "        sel_sentence = str(sentence).replace('\\n', '').strip()\n",
        "        break\n",
        "\n",
        "print(\"Candidate sentence: \" + sel_sentence)\n",
        "svoList, corefParag, corefSent, sub_rel_obj, processes = demo(question_examples[0], paragraph, answer, sel_sentence, False, True)\n",
        "print(\"Candidate sentence (coreference resolution): \" + str(corefSent))\n",
        "print(\"Related triples: \" + str(sub_rel_obj))\n",
        "svo = ''\n",
        "ind = 1\n",
        "for trp in svoList:\n",
        "    trp = str(trp).replace('\\'subject\\'', 'subject')\n",
        "    trp = str(trp).replace('\\'relation\\'', 'relation')\n",
        "    trp = str(trp).replace('\\'object\\'', 'object')\n",
        "    svo += str(ind) + '- ' + trp[1:-1] + '\\n'\n",
        "    ind += 1\n",
        "print(processes)"
      ],
      "metadata": {
        "id": "_ysHjP7qK-uA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "663909cd-9ffc-420f-c7c4-d49e71fc6d5b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Candidate sentence: It was founded in 1955.\n",
            "Ege University is a public research university in İzmir, Turkey. It was founded in 1955. It is the first university to start courses in İzmir and the fourth oldest university in Turkey. Ege University in Bornova, a district of Izmir, the third largest city in Turkey.\n",
            "Ege University is a public research university in İzmir, Turkey.. It was founded in 1955.. It is the first university to start courses in İzmir and the fourth oldest university in Turkey.. Ege University in Bornova, a district of Izmir, the third largest city in Turkey.. \n",
            "Ege University is a public research university in İzmir, Turkey.. Ege University was founded in 1955.. Ege University is the first university to start courses in İzmir and the fourth oldest university in Turkey.. Ege University in Bornova, a district of Izmir, the third largest city in Turkey.. \n",
            "['Ege University is a public research university in İzmir, Turkey. ', 'Ege University was founded in 1955. ', 'Ege University is the first university to start courses in İzmir and the fourth oldest university in Turkey. ', 'Ege University in Bornova, a district of Izmir, the third largest city in Turkey. ', '. '] Ege University is a public research university in İzmir, Turkey. Ege University was founded in 1955. Ege University is the first university to start courses in İzmir and the fourth oldest university in Turkey. Ege University in Bornova, a district of Izmir, the third largest city in Turkey.\n",
            "OpenIE entered\n",
            "Starting server with command: java -Xmx8G -cp /root/.stanfordnlp_resources/stanford-corenlp-4.1.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-6acce5a54c9b4410.props -preload openie\n",
            "Error LocationCoref:Timed out waiting for service to come alive.\n",
            "Candidate sentence (coreference resolution): ['Ege University is a public research university in İzmir, Turkey. ', 'Ege University was founded in 1955. ', 'Ege University is the first university to start courses in İzmir and the fourth oldest university in Turkey. ', 'Ege University in Bornova, a district of Izmir, the third largest city in Turkey. ', '. ']\n",
            "Related triples: \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question that has no answer"
      ],
      "metadata": {
        "id": "a2XcUbYFhpFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = question_examples[1]\n",
        "answer = answer_examples[1]\n",
        "\n",
        "svoList, corefParag, corefSent, sub_rel_obj, processes = demo(question_examples[1], paragraph, '', '', True, False)\n",
        "svo = ''\n",
        "ind = 1\n",
        "for trp in svoList:\n",
        "    trp = str(trp).replace('\\'subject\\'', 'subject')\n",
        "    trp = str(trp).replace('\\'relation\\'', 'relation')\n",
        "    trp = str(trp).replace('\\'object\\'', 'object')\n",
        "    svo += str(ind) + '- ' + trp[1:-1] + '\\n'\n",
        "    ind += 1\n",
        "print(processes) "
      ],
      "metadata": {
        "id": "7bF8MlOKLe9I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "68e61626-13d6-4020-fa5b-f69e3f6d4729"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ege University is a public research university in İzmir, Turkey. It was founded in 1955. It is the first university to start courses in İzmir and the fourth oldest university in Turkey. Ege University in Bornova, a district of Izmir, the third largest city in Turkey.\n",
            "Ege University is a public research university in İzmir, Turkey.. It was founded in 1955.. It is the first university to start courses in İzmir and the fourth oldest university in Turkey.. Ege University in Bornova, a district of Izmir, the third largest city in Turkey.. \n",
            "Ege University is a public research university in İzmir, Turkey.. Ege University was founded in 1955.. Ege University is the first university to start courses in İzmir and the fourth oldest university in Turkey.. Ege University in Bornova, a district of Izmir, the third largest city in Turkey.. \n",
            "OpenIE entered\n",
            "Starting server with command: java -Xmx8G -cp /root/.stanfordnlp_resources/stanford-corenlp-4.1.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-c8f6cf41941a4e67.props -preload openie\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PermanentlyFailedException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPermanentlyFailedException\u001b[0m                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-acbd6ac5af92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msvoList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorefParag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorefSent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_rel_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdemo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparagraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msvo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-d6bbd4623d46>\u001b[0m in \u001b[0;36mdemo\u001b[0;34m(ques, parag_examp, actual_ans, sel_sentence, no_answer, preprocess)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mcorefSent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorefParag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoreferenceResolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparag_examp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorefSent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0msvoList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenIEforSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStanfordClient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msvo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msvoList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mtriples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-13cf33c5b226>\u001b[0m in \u001b[0;36mOpenIEforSentence\u001b[0;34m(sentence, client)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'timeout'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'50000'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m'annotators'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'tokenize,ssplit,pos,depparse,parse'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     'outputFormat': 'json'}):\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0msvoText\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msvoText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-220dc6a7a30f>\u001b[0m in \u001b[0;36mannotate\u001b[0;34m(self, text, properties_key, properties, simple_format)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# https://stanfordnlp.github.io/CoreNLP/openie.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         core_nlp_output = self.client.annotate(text=text, annotators=['openie'], output_format='json',\n\u001b[0;32m---> 37\u001b[0;31m                                                properties_key=properties_key, properties=properties)\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msimple_format\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mtriples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stanfordnlp/server/client.py\u001b[0m in \u001b[0;36mannotate\u001b[0;34m(self, text, annotators, output_format, properties_key, properties, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0mrequest_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outputFormat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoreNLPClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_OUTPUT_FORMAT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# make the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_properties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequest_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"outputFormat\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"json\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stanfordnlp/server/client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, buf, properties, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrequest\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \"\"\"\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stanfordnlp/server/client.py\u001b[0m in \u001b[0;36mensure_alive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mPermanentlyFailedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Timed out waiting for service to come alive.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# At this point we are guaranteed that the service is alive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPermanentlyFailedException\u001b[0m: Timed out waiting for service to come alive."
          ]
        }
      ]
    }
  ]
}